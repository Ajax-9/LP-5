# -*- coding: utf-8 -*-
"""LP5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CkGwa9aQR4doqGbe3Bm4vpZqVdSOp1Ww

1) DFS
"""

import queue
import threading

class Graph:
    def __init__(self, V):
        self.V = V
        self.adj = [[] for i in range(V)]

    def addEdge(self, u, v):
        self.adj[u].append(v)
        self.adj[v].append(u)

    def BFS(self, start):
        q = queue.Queue()
        visited = [False] * self.V
        visited[start] = True
        q.put(start)
        while not q.empty():
            u = q.get()
            print(u, end=" ")
            for v in self.adj[u]:
                if not visited[v]:
                    visited[v] = True
                    q.put(v)

def main():
    g = Graph(7)
    g.addEdge(0, 1)
    g.addEdge(0, 2)
    g.addEdge(1, 3)
    g.addEdge(1, 4)
    g.addEdge(2, 5)
    g.addEdge(2, 6)
    print("Breadth First Search: ", end="")
    g.BFS(0)

if __name__ == "__main__":
    main()

    
    
    
    
    
    
"""2) Bubble sort"""

import numpy as np
import multiprocessing as mp

def bubble(a, n):
    for i in range(n):
        first = i % 2
        with mp.Pool(processes=mp.cpu_count()) as pool:
            pool.starmap(swap, [(a[j], a[j+1]) for j in range(first, n-1, 2) if a[j] > a[j+1]])
            
def swap(a, b):
    a, b = b, a

if __name__ == '__main__':
    n = int(input("\n enter total no of elements=>"))
    a = np.empty(n, dtype=int)
    print("\n enter elements=>")
    for i in range(n):
        a[i] = int(input())
    bubble(a, n)
    print("\n sorted array is=>")
    print()
    for i in range(n):
        print(a[i])
        
        
        
        
        
        
        
        

"""3)CUDA for vector addition"""

!apt-get install cuda

!nvcc --version

import numpy as np
from numba import cuda
# Define the kernel function to add two vectors
@cuda.jit
def vector_add(a, b, c):
# Get the thread ID
  tid = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x
# Calculate the stride length (number of threads)
  stride = cuda.gridDim.x * cuda.blockDim.x
# Loop over the elements of the vectors and add them
  for i in range(tid, a.size, stride):
    c[i] = a[i] + b[i]
# Define the main function to add two vectors
def main():
# Get the size of the vectors from the user
  N = int(input("Enter the size of the vector: "))
# Initialize the vectors with user input
  a = np.zeros(N, dtype=np.float32)
  b = np.zeros(N, dtype=np.float32)
  c = np.zeros(N, dtype=np.float32)
  print("Enter the elements of the first vector: ")
  for i in range(N):
    a[i] = float(input())
    print("Enter the elements of the second vector: ")
  for i in range(N):
    b[i] = float(input())
# Allocate the memory on the device (GPU)
  d_a = cuda.to_device(a)
  d_b = cuda.to_device(b)
  d_c = cuda.to_device(c)
# Define the number of threads per block and the number of blocks per grid
  N = 1000000
  threads_per_block = 128
  blocks_per_grid = (N + (threads_per_block - 1)) // threads_per_block
# Call the kernel function on the device (GPU)
  vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c)
# Copy the result back to the host (CPU)
  d_c.copy_to_host(c)
# Print the result
  print("Result: ", c)
# Call the main function to add two vectors
if __name__ == "__main__":
  main()









"""4) DFS """

import queue
import stack
import threading

class Graph:
    def __init__(self, V):
        self.V = V
        self.adj = [[] for i in range(V)]

    def addEdge(self, u, v):
        self.adj[u].append(v)
        self.adj[v].append(u)

    def DFS(self, start):
        s = stack.Stack()

        visited = [False] * self.V
        visited[start] = True
        s.push(start)
        while not s.empty():

            u = s.top()
            print(u, end=" ")
            s.pop()

            for v in self.adj[u]:
                if not visited[v]:
                    visited[v] = True
                    s.push(v)

if __name__ == "__main__":
    g = Graph(7)
    g.addEdge(0, 1)
    g.addEdge(0, 2)
    g.addEdge(1, 3)
    g.addEdge(1, 4)
    g.addEdge(2, 5)
    g.addEdge(2, 6)
    print("Depth First Search: ", end="")
    g.DFS(0)
    
    
    
    
    
    
    
    
    

"""DL_ASS_1_Boston"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.datasets import boston_housing
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load the Boston Housing dataset
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""Fashion_Dataset_Assignment_3"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense (128, activation='relu'), keras.layers.Dense (10, activation='softmax') ])

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = model.evaluate(test_images, test_labels) 
print('Test accuracy:', test_acc)

predictions = model.predict(test_images)
predicted_labels = np.argmax(predictions, axis=1)

num_rows = 5
num_cols = 5
num_images = num_rows * num_cols
plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2 * num_cols, 2 * i + 1) 
  plt.imshow(test_images[i], cmap='gray')
  plt.axis('off')
  plt.subplot(num_rows, 2 * num_cols, 2* i + 2)
  plt.bar(range(10), predictions[i])
  plt.xticks(range(10)) 
  plt.ylim([0, 1])
  plt.tight_layout()
  plt.title(f"Predicted label: {predicted_labels[i]}")
plt.show()










"""IMDB_ASSIGNMENT_2"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense

(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=10000)
print(f'train dataset size: {len(x_train)}')
print(f'test dataset size: {len(x_test)}')

max_len =250
x_train=pad_sequences(x_train,maxlen=max_len)
x_test=pad_sequences(x_test,maxlen=max_len)
print(f'train size: {x_train.shape}')
print(f'test size: {x_test.shape}')

model=Sequential()
model.add(Embedding(input_dim=10000,output_dim=128,input_length=max_len))
model.add(Bidirectional(LSTM(64,return_sequences=True)))
model.add(Bidirectional(LSTM(32)))
model.add(Dense(1,activation='sigmoid'))
model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(x_train,y_train,epochs=3,batch_size=150,validation_split=0.2)

loss,acc=model.evaluate(x_test,y_test,batch_size=64)
print(f"test Accuracy: {acc:4f},test loss: {loss:4f}")










"""Max and Avg"""



import sys
import numpy as np
import multiprocessing as mp

def max_reduction(arr):
    max_value = -sys.maxsize
    for i in arr:
        if i > max_value:
            max_value = i
    print("Maximum value:", max_value)

def average_reduction(arr):
    total_sum = 0
    for i in arr:
        total_sum += i
    print("Average:", total_sum / (len(arr) - 1))

if __name__ == '__main__':
    n = int(input("\nEnter total number of elements: "))
    arr = []
    print("\nEnter elements:")
    for i in range(n):
        arr.append(int(input()))
    max_reduction(arr)
    average_reduction(arr)
    
    
    
    
    
    
    
    

"""Merge Sort"""

def merge(array, low, mid, high):
    temp = [0] * 30
    i = low
    j = low
    m = mid + 1
    while j <= mid and m <= high:
        if array[j] <= array[m]:
            temp[i] = array[j]
            j += 1
        else:
            temp[i] = array[m]
            m += 1
        i += 1
    if j > mid:
        while m <= high:
            temp[i] = array[m]
            i += 1
            m += 1
    else:
        while j <= mid:
            temp[i] = array[j]
            i += 1
            j += 1
    for k in range(low, high + 1):
        array[k] = temp[k]


def mergesort(array, low, high):
    if low < high:
        mid = (low + high) // 2
        mergesort(array, low, mid)
        mergesort(array, mid + 1, high)
        merge(array, low, mid, high)


if __name__ == '__main__':
    array = []
    size = int(input("Enter total number of elements: "))
    print("Enter", size, "elements:")
    for _ in range(size):
        array.append(int(input()))
    mergesort(array, 0, size - 1)
    print("Sorted Elements as follows:")
    for element in array:
        print(element, end=" ")
    print()

    
    
    
    
    
    
    
    
"""Min and Sum"""

import sys
import numpy as np
import multiprocessing as mp

def max_reduction(arr):
    max_value = -sys.maxsize
    for i in arr:
        if i > max_value:
            max_value = i
    print("Maximum value:", max_value)

def sum_reduction(arr):
    total_sum = 0
    for i in arr:
        total_sum += i
    print("Sum:", total_sum)

if __name__ == '__main__':
    n = int(input("\nEnter total number of elements: "))
    arr = []
    print("\nEnter elements:")
    for i in range(n):
        arr.append(int(input()))
    max_reduction(arr)
    sum_reduction(arr)
